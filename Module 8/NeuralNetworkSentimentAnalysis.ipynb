{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8: Implement a Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # suppress info and warning messages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a feedforward neural network that performs sentiment classification.\n",
    "You will complete the following tasks:\n",
    "    \n",
    "1. Build your DataFrame and define your ML problem:\n",
    "    * Load the book review data set\n",
    "    * Define the label - what are you predicting?\n",
    "    * Identify the features\n",
    "2. Create labeled examples from the data set\n",
    "3. Split the data into training and test data sets\n",
    "4. Transform the training and test text data using a TF-IDF vectorizer. \n",
    "5. Construct a neural network\n",
    "6. Train the neural network\n",
    "7. Compare the neural network model's performance on the training and validation data.\n",
    "8. Improve the model's generalization performance.\n",
    "9. Evaluate the model's performance on the test data.\n",
    "10. Experiment with ways to improve the model.\n",
    "\n",
    "For this assignment, use the demo <i>Transforming Text into Features for Sentiment Analysis</i> that is contained in this unit as a reference.\n",
    "\n",
    "**<font color='red'>Note: some of the code cells in this notebook may take a while to run</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Build Your DataFrame and Define Your ML Problem\n",
    "\n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "We will work with the book review data set that contains book reviews taken from Amazon.com reviews.\n",
    "\n",
    "<b>Task</b>: In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`.\n",
    "\n",
    "You will be working with the file named \"bookReviews.csv\" that is located in a folder named \"data_NLP\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "filename = os.path.join(os.getcwd(), \"data_NLP\", \"bookReviews.csv\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the Data\n",
    "\n",
    "<b>Task</b>: In the code cell below, inspect the data in DataFrame `df` by printing the number of rows and columns, the column names, and the first ten rows. You may perform any other techniques you'd like to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1973, 2)\n",
      "Index(['Review', 'Positive Review'], dtype='object')\n",
      "                                              Review  Positive Review\n",
      "0  This was perhaps the best of Johannes Steinhof...             True\n",
      "1  This very fascinating book is a story written ...             True\n",
      "2  The four tales in this collection are beautifu...             True\n",
      "3  The book contained more profanity than I expec...            False\n",
      "4  We have now entered a second time of deep conc...             True\n",
      "5  I don't know why it won the National Book Awar...            False\n",
      "6  The daughter of a prominent Boston doctor is d...            False\n",
      "7  I was very disapointed in the book.Basicly the...            False\n",
      "8  I think in retrospect I wasted my time on this...            False\n",
      "9  I have a hard time understanding what it is th...            False\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Label\n",
    "\n",
    "This is a binary classification problem in which we will predict whether a book review is a positive or negative one. The label is the `Positive Review` column.\n",
    "\n",
    "#### Identify Features\n",
    "\n",
    "We only have one feature. The feature is the `Review` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Create Labeled Examples from the Data Set \n",
    "\n",
    "<b>Task</b>: In the code cell below, create labeled examples from DataFrame `df`. Assign the label to the variable `y`. Assign the feature to the variable `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "y = df[\"Positive Review\"]\n",
    "X = df[\"Review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Create Training and Test Data Sets\n",
    "\n",
    "<b>Task</b>: In the code cell below, create training and test sets out of the labeled examples. Create a test set that is 25 percent of the size of the data set. Save the results to variables `X_train, X_test, y_train, y_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Implement TF-IDF Vectorizer to Transform Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, you will transform the features into numerical vectors using `TfidfVectorizer`. \n",
    "\n",
    "\n",
    "<b>Task:</b> Follow the steps to complete the code in the cell below:\n",
    "\n",
    "1. Create a `TfidfVectorizer` object and save it to the variable `tfidf_vectorizer`.\n",
    "\n",
    "2. Call `tfidf_vectorizer.fit()` to fit the vectorizer to the training data `X_train`.\n",
    "\n",
    "3. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the training data `X_train`. Save the result to `X_train_tfidf`.\n",
    "\n",
    "4. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the test data `X_test`. Save the result to `X_test_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a TfidfVectorizer object \n",
    "# YOUR CODE HERE\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# 2. Fit the vectorizer to X_train\n",
    "# YOUR CODE HERE\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# 3. Using the fitted vectorizer, transform the training data \n",
    "# YOUR CODE HERE\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "# 4. Using the fitted vectorizer, transform the test data \n",
    "# YOUR CODE HERE\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing our neural network, we will have to specify the `input_shape`, meaning the dimensionality of the input layer. This corresponds to the dimension of each of the training examples, which in our case is our vocabulary size. Run the code cell below to see the vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131787\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Construct a Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.  Define Model Structure\n",
    "\n",
    "Next we will create our neural network structure. We will create an input layer, three hidden layers and an output layer:\n",
    "\n",
    "* <b>Input layer</b>: The input layer will have the input shape corresponding to the vocabulary size. \n",
    "* <b>Hidden layers</b>: We will create three hidden layers, with 64, 32, and 16 units (number of nodes) respectively. Each layer will utilize the ReLU activation function. \n",
    "* <b>Output layer</b>: The output layer will have 1 unit. The output layer will utilize the sigmoid activation function. Since we are working with binary classification, we will be using the sigmoid activation function to map the output to a probability between 0.0 and 1.0. We can later set a threshold and assume that the prediction is class 1 if the probability is larger than or equal to our threshold, or class 0 if it is lower than our threshold.\n",
    "\n",
    "Use the same approach you have taken in this course to construct a feedforward neural network model using Keras. Do the following:\n",
    "\n",
    "* Use the Keras [`Sequential` class](https://keras.io/api/models/sequential/#sequential-class) to group a stack of layers. This will be our neural network model object. Name your neural network model object ```nn_model```.    \n",
    "* Use the [`InputLayer` class](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer) to create the input layer. \n",
    "* Use the [`Dense` class](https://keras.io/api/layers/core_layers/dense/) to create each hidden layer and the output layer.\n",
    "* After creating each layer, add it to the neural network model object ```nn_model```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "L1 (Dense)                   (None, 64)                8434432   \n",
      "_________________________________________________________________\n",
      "L2 (Dense)                   (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "L3 (Dense)                   (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 8,437,057\n",
      "Trainable params: 8,437,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. Create model object\n",
    "# YOUR CODE HERE\n",
    "nn_model = keras.Sequential()\n",
    "\n",
    "# 2. Create the input layer and add it to the model object: \n",
    "# YOUR CODE HERE\n",
    "input_layer = keras.layers.InputLayer(input_shape = (vocabulary_size,), name = \"input\")\n",
    "nn_model.add(input_layer)\n",
    "\n",
    "\n",
    "# 3. Create the first hidden layer and add it to the model object:\n",
    "# YOUR CODE HERE\n",
    "hidden_layer_1 = keras.layers.Dense(units = 64, activation = \"relu\", name = \"L1\")\n",
    "nn_model.add(hidden_layer_1)\n",
    "\n",
    "# 4. Create the second layer and add it to the model object:\n",
    "# YOUR CODE HERE\n",
    "hidden_layer_2 = keras.layers.Dense(units = 32, activation = \"relu\", name = \"L2\")\n",
    "nn_model.add(hidden_layer_2)\n",
    "\n",
    "# 5. Create the third layer and add it to the model object:\n",
    "# YOUR CODE HERE\n",
    "hidden_layer_3 = keras.layers.Dense(units = 16, activation = \"relu\", name = \"L3\")\n",
    "nn_model.add(hidden_layer_3)\n",
    "nn_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "# 6. Create the output layer and add it to the model object:\n",
    "# YOUR CODE HERE\n",
    "output_layer = keras.layers.Dense(units = 1, activation = \"sigmoid\", name = \"output\")\n",
    "nn_model.add(output_layer)\n",
    "\n",
    "\n",
    "# Print summary of neural network model structure\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define the Optimization Function\n",
    "\n",
    "<b>Task:</b> In the code cell below, create a stochastic gradient descent optimizer using  `keras.optimizers.SGD()`. Specify a learning rate of 0.1. Assign the result to the variable`sgd_optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "sgd_optimizer = keras.optimizers.SGD(learning_rate = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define the Loss Function\n",
    "\n",
    "<b>Task:</b> In the code cell below, create a binary cross entropy loss function using `keras.losses.BinaryCrossentropy()`. Since our output will be a normalized probability between 0 and 1, specify that `from_logits` is `False`. Assign the result to the variable  `loss_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Compile the Model\n",
    "\n",
    "<b>Task:</b> In the code cell below, package the network architecture with the optimizer and the loss function using the `nn_model.compile()` method. Specify the optimizer, loss function and the accuracy evaluation metric as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "nn_model.compile(optimizer = sgd_optimizer, loss = loss_fn, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Fit the Model on the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define our own callback class to output information from our model while it is training. Make sure you execute the code cell below so that it can be used in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgBarLoggerNEpochs(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, num_epochs: int, every_n: int = 50):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.every_n = every_n\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every_n == 0:\n",
    "            s = 'Epoch [{}/ {}]'.format(epoch + 1, self.num_epochs)\n",
    "            logs_s = ['{}: {:.4f}'.format(k.capitalize(), v)\n",
    "                      for k, v in logs.items()]\n",
    "            s_list = [s] + logs_s\n",
    "            print(', '.join(s_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, fit the neural network model to the vectorized training data. Call the `fit()` method on the model object `nn_model` and specify the following arguments:\n",
    "\n",
    "1. The training data `X_train_tfidf` and `y_train` as arguments. Note that `X_train_tfidf` is currently of type sparce matrix. The Keras `fit()` method requires that input data be of specific types. One type that is allowed is a NumPy array. Convert `X_train_tfidf` to a NumPy array using the `toarray()` method.\n",
    "2. Use the `epochs` parameter and assign it the number of epochs.\n",
    "3. Use the `verbose` parameter and assign it the value of  0.\n",
    "4. We will use a portion of our training data to serve as validation data. Use the  `validation_split` parameter and assign it the value `0.2`\n",
    "5. Use the `callbacks` parameter and assign it a list containing our logger function: \n",
    "    `callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)]`  \n",
    "\n",
    "\n",
    "Save the results to the variable `history`.\n",
    "  \n",
    "<b>Note</b>: This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 200], Loss: 0.6928, Accuracy: 0.5106, Val_loss: 0.6935, Val_accuracy: 0.4932\n",
      "Epoch [10/ 200], Loss: 0.6900, Accuracy: 0.5537, Val_loss: 0.6934, Val_accuracy: 0.4932\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time() # start time\n",
    "\n",
    "num_epochs = 200 # epochs\n",
    "\n",
    "history = nn_model.fit(X_train_tfidf.toarray(),\n",
    "                       y_train,\n",
    "                       epochs = num_epochs,\n",
    "                       verbose = 0,\n",
    "                       validation_split = 0.2,\n",
    "                       callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)])\n",
    "\n",
    "t1 = time.time() # stop time\n",
    "\n",
    "print('Elapsed time: %.2fs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Model's Performance Over Time\n",
    "\n",
    "The code below outputs both the training loss and accuracy and the validation loss and accuracy. Let us visualize the model's performance over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(range(1, num_epochs + 1), history.history['loss'], label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(range(1, num_epochs + 1), history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Evaluate the Model's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve Model's Performance and Prevent Overfitting \n",
    "\n",
    "Neural networks can be prone to overfitting. Notice that the training accuracy is 100% but the validation accuracy is around 80%. This indicates that our model is overfitting; it will not perform as well on new, previously unseen data as it did during training. We want to have an accurate idea of how well our model will generalize to new data. Our goal is to have our training and validation accuracy scores be as close as possible.\n",
    "\n",
    "While there are different techniques that can be used to prevent overfitting, for the purpose of this exercise we will focus on two methods:\n",
    "\n",
    "1. Changing the number of epochs. Too many epochs can lead to overfitting of the training dataset, whereas too few epochs may result in underfitting.\n",
    "\n",
    "2. Adding dropout regularization. During training, the nodes of a particular layer may always become influenced only by the output of a particular node in the previous layer, causing overfitting. Dropout regularization is a technique that randomly drops a number of nodes in a neural network during training as a way of adding randomization and preventing nodes from becoming dependent on one another. Adding dropout regularization can reduce overfitting and also improve the performance of the model. \n",
    "\n",
    "<b>Task:</b> \n",
    "\n",
    "1. Tweak the variable `num_epochs` above and restart and rerun all of the cells above. Evaluate the performance of the model on the training data and the validation data.\n",
    "\n",
    "2. Add Keras `Dropout` layers after one or all hidden layers. Add the following line of code after you add a hidden layer to your model object:  `nn_model.add(keras.layers.Dropout(.25))`. The parameter `.25` is the fraction of the nodes to drop. You can experiment with this value as well. Restart and rerun all of the cells above. Evaluate the performance of the model on the training data and the validation data.\n",
    "\n",
    "\n",
    "<b>Analysis:</b> \n",
    "In the cell below, specify the different approaches you used to reduce overfitting and summarize which configuration led to the best generalization performance.\n",
    "\n",
    "Did changing the number of epochs prevent overfitting? Which value of `num_epochs` yielded the closest training and validation accuracy score? \n",
    "\n",
    "Did adding dropout layers prevent overfitting? How so? Did it also improve the accuracy score when evaluating the validation data? How many dropout layers did you add and which fraction of nodes did you drop? \n",
    "\n",
    "Record your findings in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of 200 epochs seemed to result in the highest training and validation accuracy score.\n",
    "The training loss kept increasing. Adding a dropout layer after the last layer seemed to increase performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Performance on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have improved the model, let's evaluate its performance on our test data and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, call the  `evaluate()` method on the model object `nn_model`. Specify `X_test_tfidf` and `y_test` as arguments. You must convert `X_test_tfidf` to a NumPy array using the `toarray()` method. \n",
    "\n",
    "Note: The `evaluate()` method returns a list containing two values. The first value is the loss and the second value is the accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = nn_model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "\n",
    "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best performing model that can generalize to new, previously unseen data, let us make predictions using our test data.\n",
    "\n",
    "In the cell below, we will make a prediction on our test set and receive probability predictions for every example in the test set (these values will be between 0.0 and 1.0). We will then inspect the results for the first 20 examples &mdash; We will apply a threshold to determine the predicted class for every example; we will use a threshold of 0.5. This means that if the probability is greater than 0.5, we will assume the book review is good. We will then print the actual class labels contained in `y_test` to see if our model is producing accurate predictions.\n",
    "\n",
    "<b>Task: </b> In the code cell below, do the following:\n",
    "\n",
    "1. Call the  `predict()` method on the model object `nn_model`. Specify `X_test_tfidf` as an argument. You must convert `X_test_tfidf` to a NumPy array using the `toarray()` method. Save the results to the array `probability_predictions`.\n",
    "2. Loop through the first 20 items in `probability_predictions`. These correspond to the predicted probabilities for the first 20 examples in our test set. For every item, check if the probability is greater than 0.5. If so, output:\n",
    "* the probability value in `probability_predictions`\n",
    "* the corresponding label in `y_test`. Note: convert the series `y_test` using `y_test.to_numpy()` before indexing into it.\n",
    "\n",
    "Inspect the results. How is our model performing? Is our model properly predicting whether the book reviews are good or bad reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "probability_predictions = nn_model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "print(\"Predictions for the first 20 examples:\")\n",
    "# YOUR CODE HERE\n",
    "for i in range(20):\n",
    "    if probability_predictions[i] > 0.5:\n",
    "        print(probability_predictions[i], \" \", y_test.to_numpy()[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a few of the original book review texts to get a further glimpse into how our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #1:\\n')\n",
    "\n",
    "print(X_test.to_numpy()[11])\n",
    "\n",
    "goodReview = True if probability_predictions[11] >= .5 else False\n",
    "    \n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview))\n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #2:\\n')\n",
    "\n",
    "print(X_test.to_numpy()[24])\n",
    "\n",
    "goodReview = True if probability_predictions[24] >= .5 else False\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview)) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #3:\\n')\n",
    "\n",
    "print(X_test.to_numpy()[56])\n",
    "\n",
    "goodReview = True if probability_predictions[56] >= .5 else False\n",
    "    \n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview))\n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[56]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #4:\\n')\n",
    "print(X_test.to_numpy()[102])\n",
    "\n",
    "goodReview = True if probability_predictions[102] >= .5 else False\n",
    "    \n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview))\n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[102]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Analysis\n",
    "\n",
    "Experiment with the vectorizer and neural network implementation above and compare your results every time you train the network. Pay attention to the time it takes to train the network, and the resulting loss and accuracy on both the training and test data. \n",
    "\n",
    "Below are some ideas for things you can try:\n",
    "\n",
    "* Adjust the learning rate.\n",
    "* Add more hidden layers and/or experiment with different values for the `unit` parameter in the hidden layers to change the number of nodes in the hidden layers.\n",
    "* Fit your vectorizer using different document frequency values and different n-gram ranges. When creating a `TfidfVectorizer` object, use the parameter `min_df` to specify the minimum 'document frequency' and use `ngram_range=(1,2)` to change the default n-gram range of `(1,1)`.\n",
    "\n",
    "Record your findings in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `ngram_range` greatly lowered the training and validation loss, but increased the time. The `min_df` parameter at a value of 0.1, made the performance less accurate. Decreasing the learning rate from 0.1 to 0.05 made the model more accurate, but took more time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
